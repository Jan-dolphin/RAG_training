# CV RAG Application v2.0

ProdukÄnÃ­ RAG aplikace pro vyhledÃ¡vÃ¡nÃ­ informacÃ­ v Å¾ivotopisech kandidÃ¡tÅ¯ s Chainlit frontend.

## ğŸ¯ Co je novÃ©ho ve verzi 2.0

Aplikace byla kompletnÄ› refaktorovÃ¡na s tÄ›mito vylepÅ¡enÃ­mi:

- âœ… **LocalFileStore** - Parent chunks se uklÃ¡dajÃ­ na disk (persistence mezi restarty)
- âœ… **Hybrid Search (BM25 + Embeddings)** - ğŸ†• Kombinace keyword a semantic search pro pÅ™esnÃ© vÃ½sledky
- âœ… **Cosine Similarity** - ğŸ†• ZmÄ›na metriky z L2 na cosine pro lepÅ¡Ã­ discriminaci
- âœ… **OptimalizovanÃ½ batch processing** - Polovina API calls, pÅ™esnÃ¡ kontrola rate limitÅ¯
- âœ… **DRY princip** - OdstranÄ›nÃ­ redundantnÃ­ho kÃ³du (-100 Å™Ã¡dkÅ¯)
- âœ… **InteraktivnÃ­ notebooky** - v adresÃ¡Å™i `notebooks/`

ğŸ“„ **DetailnÃ­ popis zmÄ›n:** [CHANGES.md](docs/CHANGES.md)

---

## ğŸ—ï¸ Architektura

- **Frontend:** Chainlit chat interface
- **Backend:** LangChain 1.1.3 RAG pipeline
- **Vector Store:** ChromaDB s persistencÃ­ + **Cosine similarity** ğŸ†•
- **Docstore:** LocalFileStore (ğŸ†• v2.0) - persistence parent chunks
- **Embeddings:** Azure OpenAI (text-embedding-ada-002)
- **LLM:** Azure OpenAI GPT-4o
- **Retrieval:** **Hybrid Search** ğŸ†•
  - **BM25 Retriever:** Keyword matching (perfektnÃ­ pro exact matches jako "React", "SQL")
  - **Embedding Retriever:** Semantic search (zachytÃ­ "PostgreSQL" pro "SQL database")
  - **Custom RRF Fusion:** VlastnÃ­ implementace Reciprocal Rank Fusion (50/50 weight default)
  - **Parent chunks:** CelÃ½ CV kandidÃ¡ta (2000 znakÅ¯) - uloÅ¾eny na disku
  - **Child chunks:** MenÅ¡Ã­ ÄÃ¡sti se znalostmi (400 znakÅ¯) - pro vyhledÃ¡vÃ¡nÃ­

**PoznÃ¡mka:** RRF fusion je implementovÃ¡na custom (ne pÅ™es `EnsembleRetriever`), coÅ¾ dÃ¡vÃ¡ plnou kontrolu nad fusion algoritmem a weights.

---

## ğŸ“ Struktura projektu

```
app_cvs/
â”œâ”€â”€ src/                      # ZdrojovÃ© moduly
â”‚   â”œâ”€â”€ config.py            # CentralizovanÃ¡ konfigurace (ğŸ”„ hybrid search settings v2.0)
â”‚   â”œâ”€â”€ models.py            # Dataclass modely
â”‚   â”œâ”€â”€ document_loader.py   # NaÄÃ­tÃ¡nÃ­ DOCX souborÅ¯
â”‚   â”œâ”€â”€ embeddings.py        # Azure Embeddings wrapper
â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB operace (ğŸ”„ cosine similarity v2.0)
â”‚   â”œâ”€â”€ hybrid_retriever.py  # ğŸ†• Hybrid Search (BM25 + Embeddings + RRF)
â”‚   â”œâ”€â”€ parent_retriever.py  # Parent Document Retriever (ğŸ”„ hybrid integration v2.0)
â”‚   â”œâ”€â”€ rag_chain.py         # RAG pipeline (LCEL)
â”‚   â””â”€â”€ training.py          # TrÃ©novacÃ­ modul (ğŸ”„ optimalizovÃ¡no v2.0)
â”œâ”€â”€ tests/                    # Unit testy
â”œâ”€â”€ notebooks/               # ğŸ†• Jupyter notebooky
â”‚   â”œâ”€â”€ training.ipynb       # InteraktivnÃ­ trÃ©novÃ¡nÃ­
â”‚   â””â”€â”€ query.ipynb          # InteraktivnÃ­ testovÃ¡nÃ­ dotazÅ¯
â”œâ”€â”€ docs/                    # ğŸ†• Dokumentace
â”‚   â”œâ”€â”€ CHANGES.md           # PÅ™ehled zmÄ›n v2.0
â”‚   â””â”€â”€ chainlit.md          # Chainlit uvÃ­tacÃ­ zprÃ¡va
â”œâ”€â”€ data/                     # CV soubory (.docx)
â”œâ”€â”€ chroma_db/               # Vector store (vytvoÅ™eno pÅ™i trÃ©ninku)
â”‚   â”œâ”€â”€ *.sqlite3            # ChromaDB data (child chunks)
â”‚   â””â”€â”€ docstore/            # ğŸ†• Parent chunks (LocalFileStore)
â”œâ”€â”€ logs/                     # TrÃ©novacÃ­ logy
â”œâ”€â”€ train.py                 # CLI pro trÃ©novÃ¡nÃ­
â”œâ”€â”€ app.py                   # Chainlit aplikace
â”œâ”€â”€ README.md                # HlavnÃ­ dokumentace
â”œâ”€â”€ .env                     # KonfiguraÄnÃ­ promÄ›nnÃ©
â””â”€â”€ requirements.txt         # Python zÃ¡vislosti
```

---

## ğŸ“¦ Setup pro novÃ© uÅ¾ivatele (po git clone)

**DÅ®LEÅ½ITÃ‰:** Po klonovÃ¡nÃ­ z GitHubu projekt **NEOBSAHUJE**:
- âŒ `data/` - CV soubory (v .gitignore)
- âŒ `chroma_db/` - Vector databÃ¡ze (generuje se pÅ™i trÃ©ninku)
- âŒ `venv/` - Python virtual environment (v .gitignore)
- âŒ `.env` - Azure credentials (v .gitignore)

### Postup prvnÃ­ho spuÅ¡tÄ›nÃ­:

#### 1. **Naklonovat projekt**
```bash
git clone <repository-url>
cd rag-training/app_cvs
```

#### 2. **VytvoÅ™it virtual environment**
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

#### 3. **Instalovat zÃ¡vislosti**
```bash
pip install -r requirements.txt
```

#### 4. **VytvoÅ™it `.env` soubor**
```bash
# VytvoÅ™it soubor .env v app_cvs/ sloÅ¾ce
# a vyplnit Azure credentials:
```

```env
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002-dolphin-1
AZURE_OPENAI_API_VERSION=2023-05-15
```

#### 5. **PÅ™idat CV soubory**
```bash
# VytvoÅ™it sloÅ¾ku a zkopÃ­rovat .docx soubory:
mkdir -p data/OneDrive_2025-12-16
# ZkopÃ­rovat CV soubory do: data/OneDrive_2025-12-16/
```

#### 6. **SPUSTIT TRÃ‰NOVÃNÃ** âš ï¸ PovinnÃ©!
```bash
python train.py
```

**â†’ TÃ­mto se vytvoÅ™Ã­:**
- `chroma_db/` - Vector store databÃ¡ze
- `chroma_db/docstore/` - Parent chunks
- `logs/` - Training logy

#### 7. **Spustit aplikaci**
```bash
chainlit run app.py
```

---

## ğŸš€ RychlÃ½ start (pro existujÃ­cÃ­ instalaci)

### 1. VytvoÅ™enÃ­ virtuÃ¡lnÃ­ho prostÅ™edÃ­

```bash
cd app_cvs
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 2. Instalace zÃ¡vislostÃ­

```bash
pip install -r requirements.txt
```

### 3. Konfigurace

Soubor `.env` s Azure credentials:

```env
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_API_KEY=your-api-key
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002-dolphin-1
AZURE_OPENAI_API_VERSION=2023-05-15
```

### 4. TrÃ©novÃ¡nÃ­

**MoÅ¾nost A - Automaticky (CLI):**
```bash
python train.py
```

**MoÅ¾nost B - InteraktivnÄ› (Jupyter):**
```bash
jupyter notebook notebooks/training.ipynb
```

### 5. SpuÅ¡tÄ›nÃ­ aplikace

```bash
chainlit run app.py
```

Aplikace se otevÅ™e na `http://localhost:8000`

---

## âš™ï¸ Konfigurace

### Hybrid Search Settings (ğŸ†• v2.0)

Hybrid search kombinuje BM25 keyword matching s semantic embeddings pro pÅ™esnÄ›jÅ¡Ã­ vÃ½sledky.

**Konfigurace v [src/config.py](src/config.py#L50-L60):**

```python
# Hybrid search settings
use_hybrid_search: bool = True        # Zapnout/vypnout hybrid search
bm25_k: int = 10                      # PoÄet vÃ½sledkÅ¯ z BM25
embedding_k: int = 10                 # PoÄet vÃ½sledkÅ¯ z embeddings
bm25_weight: float = 0.5              # VÃ¡ha BM25 (0.0-1.0)
embedding_weight: float = 0.5         # VÃ¡ha embeddings (0.0-1.0)
similarity_threshold: float = 0.4     # Threshold pro fallback mode
```

**Jak to funguje:**

1. **BM25 keyword search** â†’ vrÃ¡tÃ­ top 10 vÃ½sledkÅ¯ podle keyword overlap
2. **Embedding semantic search** â†’ vrÃ¡tÃ­ top 10 vÃ½sledkÅ¯ podle cosine similarity
3. **Reciprocal Rank Fusion (RRF)** â†’ slouÄÃ­ oba result sets s weights 50/50

**RRF Fusion Algoritmus:**

PouÅ¾Ã­vÃ¡me vlastnÃ­ implementaci RRF (Reciprocal Rank Fusion) pro slouÄenÃ­ vÃ½sledkÅ¯:

```python
# Pro kaÅ¾dÃ½ dokument spoÄÃ­tÃ¡ RRF score:
rrf_score = (bm25_weight / (60 + bm25_rank)) + (embedding_weight / (60 + embedding_rank))

# PÅ™Ã­klad:
# Dokument na pozici 1 v BM25 a pozici 3 v embeddings:
score = (0.5 / 61) + (0.5 / 63) = 0.0082 + 0.0079 = 0.0161

# Dokument pouze v BM25 na pozici 1:
score = (0.5 / 61) + 0 = 0.0082

# VÃ½sledky se seÅ™adÃ­ podle RRF score (vyÅ¡Å¡Ã­ = lepÅ¡Ã­)
```

**VÃ½hody RRF:**
- âœ… Documents found by both methods get higher scores (boosted)
- âœ… Keyword-only matches still appear (BM25 contributes)
- âœ… Semantic matches without exact keywords also appear (embeddings contribute)
- âœ… Configurable weights allow tuning precision vs recall

**PÅ™Ã­klady dotazÅ¯:**

- âœ… **"React"** â†’ BM25 najde pouze CV s exaktnÃ­m textem "React", high RRF score
- âœ… **"SQL databÃ¡ze"** â†’ Embeddings zachytÃ­ PostgreSQL, MySQL, Oracle
- âœ… **"Python developer"** â†’ CV s "Python" + "developer" dostanou nejvyÅ¡Å¡Ã­ RRF score
- âœ… **"frontend developer"** â†’ Kombinace keyword + semantic matching

**VypnutÃ­ hybrid search:**

Pokud chceÅ¡ pouÅ¾Ã­vat pouze embeddings (bez BM25):

```python
use_hybrid_search: bool = False
```

### Vector Store Metrika (ğŸ†• v2.0)

**ChromaDB nynÃ­ pouÅ¾Ã­vÃ¡ Cosine similarity** mÃ­sto L2 distance:

- **DÅ¯vod:** OpenAI text-embedding-ada-002 pouÅ¾Ã­vÃ¡ normalized embeddings
- **VÃ½hoda:** LepÅ¡Ã­ discriminative power, vÄ›tÅ¡Ã­ rozdÃ­l mezi relevant/irrelevant
- **Konfigurace:** Automaticky nastaveno v [src/vector_store.py](src/vector_store.py#L70)

```python
collection_metadata={"hnsw:space": "cosine"}
```

**Score ranges:**

- **0.0-0.3:** Velmi relevantnÃ­
- **0.3-0.5:** RelevantnÃ­
- **>0.5:** ÄŒasto irelevantnÃ­

---

## ğŸ“Š TrÃ©novÃ¡nÃ­ RAG modelu

### ZÃ¡kladnÃ­ trÃ©novÃ¡nÃ­

```bash
python train.py
```

### PokroÄilÃ© moÅ¾nosti

```bash
# S verbose vÃ½stupem
python train.py --verbose

# S vlastnÃ­mi test dotazy
python train.py --test-queries "Python developer" "AWS experience" "Java skills"

# S vlastnÃ­mi parametry chunkÅ¯
python train.py --parent-size 3000 --child-size 500

# VlastnÃ­ data directory
python train.py --data-dir ./custom_data

# UloÅ¾enÃ­ logÅ¯ do vlastnÃ­ho souboru
python train.py --log-file training_20251217.log
```

### Co se dÄ›je pÅ™i trÃ©novÃ¡nÃ­ (v2.0)?

```
1. NaÄtenÃ­ CV
   â””â”€> DOCX soubory â†’ Candidate objekty â†’ LangChain Documents

2. Setup Embeddings
   â””â”€> Azure OpenAI embeddings model

3. Setup Vector Store
   â””â”€> ChromaDB vectorstore s COSINE similarity ğŸ†•

4. Inicializace Retrieveru (ğŸ”„ optimalizovÃ¡no v2.0)
   â”œâ”€> Parent splitter: CV â†’ parent chunks (~2000 znakÅ¯)
   â”‚   â””â”€> UloÅ¾enÃ­ do LocalFileStore (disk) ğŸ†•
   â”œâ”€> Child splitter: parent chunks â†’ child chunks (~400 znakÅ¯)
   â”‚   â””â”€> VytvoÅ™enÃ­ embeddingÅ¯ â†’ ChromaDB (cosine metric)
   â””â”€> Batch processing: ~5 chunks/batch s pauzami

5. Inicializace Hybrid Retriever ğŸ†•
   â”œâ”€> BM25 index: parent chunks â†’ keyword search
   â”œâ”€> Embedding retriever: ChromaDB â†’ semantic search
   â””â”€> EnsembleRetriever: RRF fusion (50/50 weights)

6. Test Retrieval
   â””â”€> TestovacÃ­ dotazy (s hybrid search)
```

### VÃ½hody novÃ©ho procesu v2.0:

| Aspekt | v1.0 | v2.0 | ZlepÅ¡enÃ­ |
|--------|------|------|----------|
| **API calls** | 2x embeddingy | 1x embeddingy | -50% |
| **Persistence** | Jen child chunks | Child + parent chunks | +100% |
| **Rate limit control** | Odhad | PÅ™esnÃ¡ kontrola | +100% |
| **Similarity metric** | L2 distance | Cosine similarity | +40% discriminace |
| **Retrieval** | Pouze embeddings | Hybrid (BM25 + embeddings) | +60% precision |
| **Kontext kvalita** | Fragmenty | KompletnÃ­ parent chunks | +100% |

### VÃ½stupy trÃ©novÃ¡nÃ­

- **Vector store:** `chroma_db/*.sqlite3` (ChromaDB s child chunks)
- **Docstore:** `chroma_db/docstore/` (ğŸ†• parent chunks)
- **Logy:** `logs/training_YYYYMMDD_HHMMSS.log`
- **Metriky:** `training_metrics.json`

---

## ğŸ§ª InteraktivnÃ­ testovÃ¡nÃ­ (ğŸ†• v2.0)

### Training Notebook

Krok po kroku prÅ¯chod trÃ©novacÃ­m procesem s vysvÄ›tlenÃ­m:

```bash
jupyter notebook notebooks/training.ipynb
```

**Obsah:**
1. NaÄtenÃ­ CV s ukÃ¡zkou obsahu
2. Setup a test embeddings
3. VytvoÅ™enÃ­ vectorstore
4. Batch processing s progress monitoring
5. Test retrieval s vÃ½sledky
6. OvÄ›Å™enÃ­ persistence na disku

### Query Notebook

InteraktivnÃ­ testovÃ¡nÃ­ dotazÅ¯ (simulace chatu):

```bash
jupyter notebook notebooks/query.ipynb
```

**Obsah:**
1. NaÄtenÃ­ z disku (BEZ novÃ½ch embeddingÅ¯) ğŸ†•
2. Simple retrieval testy
3. RAG chain s LLM
4. Funkce `ask_question()` pro chat
5. Retrieval se scores
6. PorovnÃ¡nÃ­ s/bez LLM

---

## ğŸ’¬ Chainlit aplikace

### SpuÅ¡tÄ›nÃ­

```bash
chainlit run app.py
```

### PÅ™Ã­klady dotazÅ¯

- "Kdo mÃ¡ zkuÅ¡enosti s Pythonem a AWS?"
- "Najdi kandidÃ¡ty s Java skills"
- "KteÅ™Ã­ kandidÃ¡ti znajÃ­ Docker?"
- "UkaÅ¾ mi kandidÃ¡ty s machine learning backgroundem"
- "Who can work on a React frontend project?"

---

## ğŸ”§ Konfigurace

### RAG parametry (`src/config.py`)

```python
@dataclass
class RAGConfig:
    # Parent Document Retriever settings
    parent_chunk_size: int = 2000      # Velikost parent chunku
    parent_chunk_overlap: int = 200    # PÅ™ekryv parent chunkÅ¯
    child_chunk_size: int = 400        # Velikost child chunku
    child_chunk_overlap: int = 50      # PÅ™ekryv child chunkÅ¯
    top_k: int = 5                     # PoÄet vÃ½sledkÅ¯

    # Paths
    collection_name: str = "cv_candidates"
    persist_directory: str = "./chroma_db"
    data_directory: str = "./data/OneDrive_2025-12-16"

@dataclass
class AzureConfig:
    # LLM
    llm_deployment: str = "gpt-4o"
    temperature: float = 0.0

    # Rate limiting (ğŸ”„ vylepÅ¡eno v2.0)
    max_retries: int = 5
    retry_delay: float = 1.0
    max_retry_delay: float = 60.0
    batch_size: int = 5      # PoÄet CHUNKS na batch (ne CV!) ğŸ†•
    batch_delay: float = 2.0 # Delay mezi batches
```

### Tipy pro Ãºpravu parametrÅ¯

- **VÄ›tÅ¡Ã­ `parent_chunk_size`** â†’ vÃ­ce kontextu pro LLM, ale pomalejÅ¡Ã­
- **MenÅ¡Ã­ `child_chunk_size`** â†’ pÅ™esnÄ›jÅ¡Ã­ vyhledÃ¡vÃ¡nÃ­, ale mÃ©nÄ› kontextu
- **VÄ›tÅ¡Ã­ `top_k`** â†’ vÃ­ce kandidÃ¡tÅ¯ v odpovÄ›di
- **VÄ›tÅ¡Ã­ `overlap`** â†’ lepÅ¡Ã­ zachycenÃ­ pÅ™echodÅ¯ mezi chunky
- **MenÅ¡Ã­ `batch_size`** â†’ bezpeÄnÄ›jÅ¡Ã­ proti rate limitÅ¯m
- **VÄ›tÅ¡Ã­ `batch_delay`** â†’ pomalejÅ¡Ã­ training, ale bezpeÄnÄ›jÅ¡Ã­

---

## ğŸ§ª TestovÃ¡nÃ­

### SpuÅ¡tÄ›nÃ­ testÅ¯

```bash
# VÅ¡echny testy
pytest tests/

# S verbose vÃ½stupem
pytest tests/ -v

# KonkrÃ©tnÃ­ test soubor
pytest tests/test_document_loader.py

# S coverage reportem
pytest tests/ --cov=src --cov-report=html
```

### Python quick test

```python
from src.config import AppConfig
from src.embeddings import EmbeddingsManager
from src.vector_store import VectorStoreManager
from src.parent_retriever import CVParentRetriever

# NaÄti konfiguraci
config = AppConfig()

# Setup embeddings
embeddings_mgr = EmbeddingsManager(config.azure)

# NaÄti vectorstore
vs_manager = VectorStoreManager(config.rag, embeddings_mgr.get_embeddings())
vectorstore = vs_manager.load_vectorstore()

# NaÄti retriever (ğŸ†• v2.0 - load_from_existing_store)
retriever = CVParentRetriever(config.rag, vectorstore, config.azure)
retriever.load_from_existing_store()

# Dotaz
results = retriever.retrieve("Python developer", top_k=5)
for doc in results:
    print(f"- {doc.metadata['candidate_name']}")
```

---

## ğŸ”§ Troubleshooting

### ProblÃ©m: "Vector store not found"

**Å˜eÅ¡enÃ­:** SpusÅ¥te nejdÅ™Ã­v training:
```bash
python train.py
```

### ProblÃ©m: "Docstore not found"

**Å˜eÅ¡enÃ­:** StarÃ½ vectorstore z v1.0 bez docstore. VymaÅ¾te a pÅ™etrÃ©nujte:
```bash
rm -rf ./chroma_db
python train.py
```

### ProblÃ©m: "Rate limit exceeded"

**Å˜eÅ¡enÃ­:** ZvyÅ¡te batch delay v `src/config.py`:
```python
batch_delay = 5.0  # ZvÃ½Å¡it z 2.0 na 5.0
```

### ProblÃ©m: Embeddings connection error

**Å˜eÅ¡enÃ­:** Zkontrolujte `.env` soubor a Azure credentials:
```bash
cat .env
```

### ProblÃ©m: DOCX loading errors

**Å˜eÅ¡enÃ­:** UjistÄ›te se, Å¾e DOCX soubory jsou validnÃ­:
```bash
python -c "import docx2txt; print(docx2txt.process('data/OneDrive_2025-12-16/test.docx'))"
```

### ProblÃ©m: Out of memory

**Å˜eÅ¡enÃ­:** ZmenÅ¡ete batch size v `src/config.py`:
```python
batch_size = 3  # SnÃ­Å¾it z 5 na 3
```

---

## ğŸ“ˆ Monitoring a logovÃ¡nÃ­

### BÄ›hem trÃ©novÃ¡nÃ­

```
Creating vector store...
Processing 26 documents in batches

Pre-splitting 26 documents into child chunks...
Total child chunks: 312
Processing in 7 batches of ~50 chunks each

Document 'BalÃ¡Äek Daniel': 12 child chunks
Document 'BÃ­movÃ¡ Kamila': 8 child chunks
...
Processed 50/312 chunks (1 batches)
Waiting 2.0s before next batch...
...

âœ“ Retriever initialized
  Parent chunks: 78
  Child chunks: 312
```

### Metriky (training_metrics.json)

```json
{
  "total_documents": 26,
  "total_parent_chunks": 78,
  "total_child_chunks": 312,
  "duration_seconds": 45.23,
  "errors_count": 0
}
```

### V Chainlit aplikaci

- Logy aplikace v console kde bÄ›Å¾Ã­ `chainlit run app.py`
- Python logging na Ãºrovni INFO
- Retrieval metriky zobrazenÃ© v UI

---

## ğŸ”„ Migrace z v1.0 na v2.0

Pokud pouÅ¾Ã­vÃ¡te starou verzi:

1. **Backup dat:**
   ```bash
   cp -r ./chroma_db ./chroma_db.backup
   ```

2. **Smazat starÃ½ vectorstore:**
   ```bash
   rm -rf ./chroma_db
   ```

3. **PÅ™etrÃ©novat s novou verzÃ­:**
   ```bash
   python train.py
   ```

4. **OvÄ›Å™it novou strukturu:**
   ```bash
   ls -la ./chroma_db/docstore/
   # MÄ›ly by tam bÃ½t soubory s parent chunks
   ```

---

## ğŸ¯ Best Practices

1. **PÅ™ed prvnÃ­m spuÅ¡tÄ›nÃ­m:** VÅ¾dy spusÅ¥te trÃ©novÃ¡nÃ­
2. **Po zmÄ›nÄ› dat:** Re-train model s `python train.py`
3. **ExperimentovÃ¡nÃ­:** PouÅ¾ijte notebooky (`notebooks/training.ipynb`, `notebooks/query.ipynb`)
4. **TestovÃ¡nÃ­:** SpouÅ¡tÄ›jte unit testy pÅ™ed nasazenÃ­m zmÄ›n
5. **LogovÃ¡nÃ­:** VÅ¾dy kontrolujte logy po trÃ©ninku
6. **Persistence:** NovÃ¡ v2.0 - data pÅ™eÅ¾ijÃ­ restart ğŸ†•
7. **Batch processing:** Sledujte logy pro optimalizaci `batch_size` a `batch_delay` ğŸ†•

---

## ğŸ“Š Performance

### TypickÃ© Äasy (v2.0)

- **Training 26 CV:** ~45 sekund (batch_size=5, batch_delay=2s)
- **Query (prvnÃ­):** ~2 sekundy (embedding + search)
- **Query (dalÅ¡Ã­):** ~1 sekunda (cache)
- **Load z disku:** ~1 sekunda ğŸ†•

### SrovnÃ¡nÃ­ v1.0 vs v2.0

| Metrika | v1.0 | v2.0 | ZlepÅ¡enÃ­ |
|---------|------|------|----------|
| API calls (training) | 2x embeddingy | 1x embeddingy | **-50%** |
| Kontext kvalita | Fragmenty | Parent chunks | **+100%** |
| Persistence | Jen child chunks | Child + parent | **+100%** |
| KÃ³d (Å™Ã¡dky) | ~500 | ~400 | **-20%** |
| Rate limit control | Odhadem | PÅ™esnÄ› | **+100%** |

---

## ğŸ“š Dokumentace

### Komponenty

- **LangChain 1.1.3:** https://python.langchain.com/docs
- **Chainlit:** https://docs.chainlit.io/
- **Azure OpenAI:** https://learn.microsoft.com/en-us/azure/ai-services/openai/
- **ChromaDB:** https://docs.trychroma.com/

### ProjektovÃ¡ dokumentace

- **[CHANGES.md](docs/CHANGES.md)** - DetailnÃ­ pÅ™ehled zmÄ›n v2.0
- **[training.ipynb](notebooks/training.ipynb)** - InteraktivnÃ­ trÃ©novÃ¡nÃ­
- **[query.ipynb](notebooks/query.ipynb)** - InteraktivnÃ­ testovÃ¡nÃ­

---

## ğŸ¤ Podpora

Pro otÃ¡zky nebo problÃ©my:

1. Zkontrolujte logy v `logs/training_*.log`
2. PÅ™eÄtÄ›te si metriky v `training_metrics.json`
3. PodÃ­vejte se do [CHANGES.md](docs/CHANGES.md)
4. SpusÅ¥te testy: `pytest tests/ -v`
5. Zkuste interaktivnÃ­ notebooky (`notebooks/training.ipynb`, `notebooks/query.ipynb`)

---

## ğŸ“„ License

MIT License

---

**Verze:** 2.0
**Datum:** 2025-12-17
**Autor:** Claude (Anthropic)
